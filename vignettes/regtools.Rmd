---
title: "Introduction to regtools"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to regtools}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo=FALSE}
library(regtools)
```

**regtools** aims to facilitate the manipulation, analysis and visualization of health and population registers and capitalize on their characteristics.

Researchers with access to microdata from health and/or administrative registers in Norway can use regtools to streamline their initial analytical process. The package includes functions for data pre-processing, linkage, and the computation of relevant statistics and visualizations, such as stratified frequencies, incidence and prevalence rates.

This vignette introduces regtool's main functions and shows examples of how to use them with individual-level data. The package also includes some helper functions that can aid researchers working with registry data in Norway address specific problems, more information in the `vignette("h-functions")` and `vignette("other-useful-fun")`. 

## Example datasets

To exemplify the main functions of the package, we have included a couple of illustrative simulated datasets. These datasets are also used in the examples specified in the functions' documentation.

The dataset `diag_df` is a tibble with simulated individual-level diagnostic data. It is documented in `?diag_df`
```{r diagdf, echo=TRUE}
str(diag_df)
```

The dataset `var_df` is a tibble with simulated individual-level time-varying data. It is documented in `?var_df`
```{r vardf, echo=TRUE}
str(var_df)
```

The dataset `invar_df` is a tibble with simulated individual-level time-invariant data. It is documented in `?invar_df`
```{r invardf, echo=TRUE}
str(invar_df)
```

All the datasets above have been created using the `synthetic_dataset()` function included in this package. The exact code used to create them can be found in the folder `/data-raw` in the package's source code.

## Logging 

To facilitate reproducibility and transparency across research projects, the majority of functions in regtools create log files that document their internal data processing, warnings/errors and corresponding outputs. Similarly, each function provides clear console feedback after it executes important operations (filter, select, join, etc). 

It is possible to either provide the path to an already existing .log file or set the argument `log_path = NULL` (default) to create a new /log directory and .log file for each function. For instance, the `read_diag_data()` with a `NULL` log_path will first check in the working directory for a `/log` directory. In case it does not already exist, the `/log` directory is created in the current wd and a <read_diag_data_dd_mm_yyyy.log> file is initialized. 

For the next examples, a temporary log file is used: 

```{r log-setup, echo=TRUE}
log_file <- tempfile()
cat("Vignette log file", file = log_file)
```

## Data reading and validation

Generally, researchers working with Norwegian individual-level registry data will find their datasets divided and delivered in three different categories:

-   **Diagnostic data:** at least including unique personal identifier (ID), date of diagnostic event, and diagnosis code (such as ICD-10 or ICP-2). For instance, datasets from NPR (Norwegian Patient Registry) or KUHR.

-   **Time-invariant:** sociodemographic data such as date of birth, immigration background, etc.

-   **Time-varying data:** sociodemographic data such as place of residence or marital status. This type of information usually is updated quarterly or yearly in administrative registries.

Assuming a researcher would want to compute sex-stratified prevalence rates for a certain diagnosis, at least they would need a diagnostic dataset and a time-invariant dataset (including sex).

The functions `read_demo_data()` and `read_diag_data()` check the dataset's minimum requirements, as well as giving a quick summary of the dataset's structure. For example, in the case of *time-invariant* data, it is assumed that each row in the dataset corresponds to a individual identified by a unique personal identified. To check this type of minimum requirements, it is necessary to specify some of the column names corresponding to the id or date variables (see the function documentation for more info).

**Note**: these functions only check for minimum requirements, your real datasets can contain additional variables/columns. For example, NPR sometimes includes sex and age information for each individual in the same diagnostic dataset. 

These functions support common data file formats that researchers might realistically encounter in data deliveries, such as CSV, SAV and *parquet* files. Parquet files are specially useful when working with very large data, as the storage and data processing is more efficient than other types of file formats (e.g. CSV files). As explained in the `vignette("parquet-files")`, it is encouraged to use parquet files for large and larger-than-memory data within regtools. The vignette also includes general instructions on how to write/save parquet datasets. 

To read and validate the minimum requirements of a CSV file containing diagnostic data: 
```{r read-diag, echo=TRUE}
diag_csv <- system.file("extdata", "diag_data.csv", package = "regtools")

diag_data_validated <- regtools::read_diag_data(
  diag_csv,
  id_col = "id",
  date_col = "diag_year",
  log_path = log_file
)
```

Similarly, to read and validate the minimum requirements of time-invariant dataset stored as a CSV file: 
```{r read-demo, echo=TRUE}
demo_csv <- system.file("extdata", "invar_data.csv", package = "regtools")

demo_data_validated <- read_demo_data(
  demo_csv,
  data_type = "t_invariant",
  id_col = "id",
  log_path = log_file
)
```

In the case of CSV, RDS/RDA and SAV files, the resulting output is a tibble that can be further passed as input to other functions in the package. For parquet files/datasets, the output is an ArrowObject that can be used as input in the filtering functions described in the section below. For more information about ArrowObject, please consult the documentation from the package `arrow`.

## Data filtering

Usually, individual-level registry data has a large number of observations which makes it cumbersome to manipulate and prepare for analysis. For that reason, it is advantageous to follow a filter first approach and filter out the non-relevant variables and observations. 

As mentioned in the `vignette("parquet-files")`, parquet files are more efficient than other type of files at performing operations on large datasets. Therefore, both the `filter_diag()` and `filter_demo()` functions are more memory efficient when providing an ArrowObject as an input (see section above). 

Regardless of the input's data type (ArrowObject or tibble), the output of these functions are tibbles. It is assumed that after the initial filtering of both diagnostic and sociodemographic data, the datasets will be smaller and easier for users to manipulate as in-memory tibbles. 

### Filter diagnostic data 

Due to the distinct characteristics of diagnostic and time-varying/time-invariant datasets, there are two filtering functions: `filter_diag()` and `filter_demo()`. The former validates that the ICD-10 or ICPC-2 codes/family or codes are currently valid in the desired classification system. If they are valid, it filters the given diagnostic dataset to keep only the observations with the relevant ICD-10 or ICPC-2 codes. Additionally, it is possible to filter by date of the diagnostic event and remove all rows with missing data. 

For example, to keep only the observations that either have the ICD-10 code *F840* or *F841*, between the years of 2016 and 2017: 

```{r filter-diag-codes, echo = TRUE}
filtered_diag_codes <- filter_diag(
  data = diag_df,
  codes = c("F840", "F841"),
  classification = "icd",
  id_col = "id",
  code_col = "code",
  date_col = "diag_year",
  diag_dates = c("2016", "2017"),
  log_path = log_file
)
```

Alternatively, it is also possible to filter by pattern/family of codes. For example, to keep the observations with all valid ICD-10 codes starting with *F45* or *F84*: 

```{r filter-diag-pattern, echo = TRUE}
filtered_diag_pattern <- filter_diag(
  data = diag_df,
  pattern_codes = c("F45", "F84"),
  classification = "icd",
  id_col = "id",
  code_col = "code",
  log_path = log_file
)
```

To further refine the filter of a diagnostic dataset, the `curate_diag()` provides some additional options. It is important to highlight that this function only supports data frames (preferably tibbles) as input. 

To keep only cases (identified by an unique person identifier) with at least 1 diagnostic events, and summarizing information by their first-time diagnosis: 

```{r curate-diag, echo = TRUE}
curated_diag <- curate_diag(
  data = filtered_diag_pattern,
  min_diag = 1,
  first_diag = TRUE,
  id_col = "id",
  code_col = "code",
  date_col = "diag_year",
  log_path = log_file
)
```


### Filter time-varying and time-invariant data 

Similar to the filtering of diagnostic data, `filter_demo()` aids with the filtering of both time-varying and time-invariant datasets. As previously mentioned, this function also supports ArrowObject as input data. 

For example, to only keep observations where the individuals have resided in the municipality "0815" between the years 2012 and 2015: 

```{r filter-var, echo = TRUE}
filtered_var <- filter_demo(
  data = var_df,
  data_type = "t_variant",
  filter_param = list("year_varying" = c(2012:2015), "varying_code" = c("0815")),
  log_path = log_file
)
```
 
 In the case of time-invariant data, to keep only individuals with year of birth between 2010-2018 and reason of immigration "ARB", "NRD" or "UKJ": 

```{r filter-invar, echo = TRUE}
filtered_invar <- filter_demo(
  data = invar_df, data_type = "t_invariant",
  filter_param = list("y_birth" = c(2010:2018), "innvandringsgrunn" = c("ARB", "NRD", "UKJ")),
  rm_na = FALSE,
  log_path = log_file
)
```

## Linkage

Many of the analyses typically done with registry data require the linkage of different registers to access both health and sociodemographic information (e.g. stratified prevalence rates). After filtering the datasets to keep only relevant observations, it is easier to then link them using the individuals unique personal identifier present in each dataset. Depending on the type of analysis and relevant variables, it might not be necessary to link any datasets. 

The `link_diag_demo()` function can aid with the linkage process conditioned that all datasets share the same IDs to uniquely identify individuals. To link both the already filtered diagnostic and time-invariant datasets: 

```{r link-data, echo = TRUE}
linked_diag_inv <- link_diag_demo(
  data_diag = curated_diag,
  data_demo_inv = filtered_invar,
  id_col = "id",
  log_path = log_file
)
```

As part of the linking process, the function provides some useful information about the number of matched cases (individuals) and summary information of the new linked dataset. The resulting dataset from `link_diag_demo()` is a minimal dataset that contains all relevant observations to perform further analyses.

## Analysis

Until this point, all the functions described are concerned with general data preparation and manipulation that researchers working with this type of individual-level data have to perform to some degree. Considering that Norwegian registry health data is often use for epidemiology research, the functions described in the next section are specific to descriptive epidemiology and public health monitoring. 

Two of the most common measures in descriptive epidemiology are incidence and prevalence rates. Consequently, regtools includes the functions `calculate_prevalence()` and `calculate_incidence()` to aid with these analyses. Both functions require as input a filtered linked dataset with relevant observations (output from `link_diag_demo`), and the population counts stored in a different dataset. 

As diagnoses under F84 and F45 are considered to be chronic or persistent, it is assumed for the next examples that once an individual is diagnosed, then they will always be part of the case group. Then, to compute the prevalence of F84 and F45 (and subcodes) given in the time period 2012-2020 to individuals born between 2010-2018 and with reason of immigration "ARB", "NRD" or "UKJ": 

```{r prevalence, echo = TRUE}

pop_df <- tibble::tibble(year = "2012-2020", population = 30024)
linked_diag_inv <- linked_diag_inv |> dplyr::rename("year"= "y_diagnosis_first")

prevalence_df <- calculate_prevalence(linked_diag_inv,
  id_col = "id",
  date_col = "year",
  pop_data = pop_df,
  pop_col = "population",
  time_p = c(2012,2020),
  CI = TRUE,
  CI_level = 0.95,
  only_counts = FALSE,
  suppression = TRUE,
  suppression_threshold = 10,
  log_path = log_file)

prevalence_df

```

Besides computing prevalence rates based on the provided linked dataset, `calculate_prevalence()` can also provide confidence intervals and suppression of low counts to adhere to confidentiality rules. Furthermore, there is an option to output only the case counts which can be useful for other type of statistical analyses (e.g. chi square tests). 

In a lot of cases, it is relevant to compute counts or rates stratified by certain groupings. It is also possible to specify this, as long as the population dataset includes the necessary information (warning if it does not have a one-to-one match): 
```{r prevalence-strat, echo = TRUE}
pop_df <- tidyr::expand_grid(year = "2012-2020",
                               sex = as.factor(c(0, 1)),
                               innvandringsgrunn = c("ARB", "UKJ", "NRD")) |>
    dplyr::mutate(population = floor(runif(dplyr::n(), min = 3000, max = 4000)))


prevalence_df_strat <- calculate_prevalence(linked_diag_inv,
  id_col = "id",
  date_col = "year",
  pop_data = pop_df,
  pop_col = "population",
  grouping_vars = c("sex","innvandringsgrunn"),
  time_p = c(2012,2020),
  CI = TRUE,
  CI_level = 0.95,
  only_counts = FALSE,
  suppression = TRUE,
  suppression_threshold = 10,
  log_path = log_file)

prevalence_df_strat

```


Often in descriptive epidemiology, researchers are interested in the evolution of prevalence of a certain disease through time. For that purpose, the `compute_prevalence_series()` can compute prevalence rates for a series of periods of time: 


```{r prevalence-series, echo = T, results = 'hide', message=FALSE}

# Silenced CLI output for example 

pop_df <- tidyr::expand_grid(year = c("2012-2014", "2015-2017", "2018-2020"),
                             sex = as.factor(c(0, 1)),
                             innvandringsgrunn = c("ARB", "UKJ", "NRD")) |>
  dplyr::mutate(population = floor(runif(dplyr::n(), min = 2000, max = 4000)))

prevalence_df_series <- calculate_prevalence_series(linked_diag_inv,
  time_points = list(c(2012,2014), c(2015,2017), c(2018,2020)),
  id_col = "id",
  date_col = "year",
  pop_data = pop_df,
  pop_col = "population",
  grouping_vars = c("sex", "innvandringsgrunn"),
  only_counts = FALSE,
  suppression = TRUE,
  suppression_threshold = 1,
  CI = TRUE,
  CI_level = 0.95,
  log_path = log_file)

prevalence_df_series
```

## Visualize 

In the case of comparison between different groups or changes through time, it is useful to visualize the prevalence/incidence rates. For that purpose, the function `plot_rates()` can create some ready-to-use plots with a consistent visual theme: 

```{r plot-rates, fig.width = 5.5, fig.height = 5.5}

plot_line <- plot_rates(prevalence_df_series,
                        date_col = "year",
                        rate_col = "prev_rate",
                        plot_type = "line",
                        grouping_var = "sex",
                        facet_var = "innvandringsgrunn",
                        palette = "fhi_colors",
                        CI_lower = "ci_results_lower",
                        CI_upper = "ci_results_upper",
                        annotated_line = "2015-2017",
                        plot_title = "Prevalence by sex and reason of immigration",
                        x_name = "Year",
                        start_end_points = TRUE)

plot_line

```

As the output of this function is a ggplot object, you can further modify using the `ggplot2` suite of functions. 

```{r}

plot_line + ggplot2::labs(subtitle = "Norway, individuals born between 2010-2018")
  
```

